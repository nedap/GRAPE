{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libs and Change WD:\n",
    "\n",
    "Import the libraries we need, and also change the working directory to the directory containing the PartNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# Use pathlib\n",
    "current_dir = Path.cwd()\n",
    "target_dir = Path('/srv/healthcare/datascience/data/part-net')\n",
    "\n",
    "print(f'Current Working Directory: {current_dir}')\n",
    "if current_dir != target_dir:\n",
    "    print('Changing directory')\n",
    "    os.chdir(target_dir)\n",
    "    print(f'New Working directory: {Path.cwd()}')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Unique Samples:\n",
    "\n",
    "We would like to get some insight into the data distribution, therefore extract the number of samples per class."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def process_sample(sample):\n",
    "    \"\"\"\n",
    "    Process a single sample, loading its class information.\n",
    "\n",
    "    :param sample: The sample directory name.\n",
    "    :return: A tuple containing the class name and count.\n",
    "    \"\"\"\n",
    "    meta_data_path = Path(f'data_v0/{sample}/meta.json')\n",
    "    try:\n",
    "        with open(meta_data_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            class_name = data['model_cat']\n",
    "            return (class_name, 1)\n",
    "    except:\n",
    "        return ('_error', 1)\n",
    "\n",
    "with Pool() as pool:\n",
    "    results = pool.map(process_sample, os.listdir('data_v0'))\n",
    "\n",
    "class_info = defaultdict(int)\n",
    "loading_errors = 0\n",
    "\n",
    "for class_name, count in results:\n",
    "    if class_name == '_error':\n",
    "        loading_errors += count\n",
    "    else:\n",
    "        class_info[class_name] += count\n",
    "\n",
    "def plot_sample_dist(data):\n",
    "    \"\"\"\n",
    "    Plot the class distribution in a visually appealing way.\n",
    "\n",
    "    :param data: DataFrame containing the class and samples information.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(18, 8), dpi=300)\n",
    "    sns.set(style='whitegrid', rc={'axes.facecolor':'#EAEAF2', \n",
    "                               'grid.color':'#FFFFFF', \n",
    "                               'axes.edgecolor':'#FFFFFF', \n",
    "                               'grid.linewidth': 2.5})  # Increase grid line width\n",
    "    sns.set_context(\"talk\")\n",
    "\n",
    "    ax = sns.barplot(x='samples', y='class', data=data, color='#ffa94d')\n",
    "    ax.set_xlabel('Samples', fontsize=30, weight='bold')\n",
    "    ax.set_ylabel('Class', fontsize=30, weight='bold')\n",
    "    ax.tick_params(axis='x', labelsize=24)\n",
    "    ax.tick_params(axis='y', labelsize=20)\n",
    "\n",
    "    # Format x-axis labels to be more readable\n",
    "    ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f'{int(x/1000)}K'))\n",
    "\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        # spine.set_color('black')\n",
    "        spine.set_linewidth(1.5)\n",
    "\n",
    "    plt.tight_layout()  # Adjust the padding here\n",
    "    plt.subplots_adjust(right=0.85)\n",
    "\n",
    "    # Saving the plot\n",
    "    plt.savefig(\"/../../../../../home/toy-problem/notebooks/img/partnet_data_dist.png\")\n",
    "\n",
    "\n",
    "print(f'Loading errors: {loading_errors}')\n",
    "\n",
    "sorted_class_info = sorted(class_info.items(), key=lambda x: x[1], reverse=True)\n",
    "df = pd.DataFrame(sorted_class_info, columns=['class', 'samples'])\n",
    "plot_sample_dist(df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new dataset consisting of only one class ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(sample):\n",
    "    meta_data_path = os.path.join('data_v0', sample, 'meta.json')\n",
    "    try:\n",
    "        with open(meta_data_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            return (sample, data['model_cat'])\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def copy_sample(args):\n",
    "    sample, target_dir = args\n",
    "    src_dir = os.path.join('data_v0', sample)\n",
    "    dst_dir = os.path.join(target_dir, sample)\n",
    "    shutil.copytree(src_dir, dst_dir)\n",
    "\n",
    "def copy_samples_of_class(target_class, target_dir):\n",
    "    with Pool() as pool:\n",
    "        # Get the class of each sample\n",
    "        sample_classes = pool.map(get_class, os.listdir('data_v0'))\n",
    "\n",
    "        # Filter out None values before unpacking\n",
    "        sample_classes = [sc for sc in sample_classes if sc is not None]\n",
    "        \n",
    "        # Filter to get the samples of the target class\n",
    "        target_samples = [sample for sample, class_ in sample_classes if class_ == target_class]\n",
    "        \n",
    "        # Copy the target samples to the target directory\n",
    "        pool.map(copy_sample, [(sample, target_dir) for sample in target_samples])\n",
    "\n",
    "# Specify the class you're interested in and the directory to copy to\n",
    "target_class = 'Table'\n",
    "target_dir = 'data_tbl_v0'\n",
    "\n",
    "# Call the function to copy the samples\n",
    "if os.path.exists(target_dir):\n",
    "    print(f'the folder {target_dir} already exists.')\n",
    "else:\n",
    "    copy_samples_of_class(target_class, target_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the unimportant files from a dataset\n",
    "\n",
    "After creating the single class datasets, I noticed some files are redundent. \n",
    "\n",
    "| Filename                           | Description                                                         |\n",
    "|------------------------------------|---------------------------------------------------------------------|\n",
    "| `result.json`                      | JSON storing part hierarchical trees from raw user annotation       |\n",
    "| `result_after_merging.json`        | JSON storing part hierarchical trees post semantics merging (final) |\n",
    "| `meta.json`                        | JSON storing all related meta-information                           |\n",
    "| `tree_hier.html`                   | HTML visualization for hierarchical annotation (pre-merging)         |\n",
    "| `tree_hier_after_merging.html`     | HTML visualization for hierarchical annotation (post-merging)        |\n",
    "\n",
    "### Directories and Their Contents:\n",
    "\n",
    "- **objs/**: Contains part obj files indexed by `result.json`. Note:\n",
    "    - Parts here are not final; refer to `result.json`.\n",
    "    - Individual obj files might not make sense in isolation.\n",
    "    - Files prefixed with `original-` are from the original ShapeNet model.\n",
    "    - Files prefixed with `new-` are smoothed/cut-out in PartNet annotation procedure.\n",
    "\n",
    "- **part_renders/** and **part_renders_after_merging/**:\n",
    "    - Contain rendered images supporting respective HTML visualizations.\n",
    "\n",
    "- **point_sample/**: Contains data for point cloud learning.\n",
    "    - `pts-10000.txt`: Point cloud sampled from the combination of part meshes under `objs/`.\n",
    "    - `label-10000.txt`: Labels are the id in `result.json`.\n",
    "    - `sample-points-all-pts-nor-rgba-10000.txt`: Point cloud from whole ShapeNet model with labels transferred from `label-10000.txt`.\n",
    "    - `sample-points-all-label-10000.txt`: Labels propagated to `sample-points-all-pts-nor-rgba-10000.txt`.\n",
    "\n",
    "### Notes:\n",
    "\n",
    "- Files labeled \"after_merging\" are final data files and should be prioritized for use.\n",
    "- Object files in the `objs/` directory are not important for our purposes and have been omitted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = ['tree_hier.html', 'objs/', 'objs-normalized-as-shapenetv1/', \n",
    "             'parts_render/', 'point_sample/ply-10000.ply', 'point_sample/pts-10000.pts', \n",
    "             'point_sample/label-10000.txt', 'point_sample/pts-10000.txt']\n",
    "dataset = 'data_table'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b16ea4f6e7e412fae10efc5d301f90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing subdirectories:   0%|          | 0/9906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def remove_items(root_dir: str, items_to_remove: list):\n",
    "    for sub_dir_name in tqdm(os.listdir(root_dir), desc=\"Processing subdirectories\"):\n",
    "        sub_dir_path = os.path.join(root_dir, sub_dir_name)\n",
    "        if os.path.isdir(sub_dir_path):\n",
    "            for item in items_to_remove:\n",
    "                item_path = os.path.join(sub_dir_path, item)\n",
    "                if os.path.exists(item_path):\n",
    "                    try:\n",
    "                        if os.path.isdir(item_path):\n",
    "                            shutil.rmtree(item_path)\n",
    "                        else:\n",
    "                            os.remove(item_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error removing {item_path}: {e}\")\n",
    "\n",
    "# Usage\n",
    "dataset_dir = os.path.join(os.getcwd(), dataset)  # Assuming 'dataset' is the folder name\n",
    "remove_items(dataset_dir, to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are left with the following files for each sample:\n",
    "\n",
    "| Filename                           | Description                                                         |\n",
    "|------------------------------------|---------------------------------------------------------------------|\n",
    "| `result.json`                      | JSON storing part hierarchical trees from raw user annotation       |\n",
    "| `result_after_merging.json`        | JSON storing part hierarchical trees post semantics merging (final) |\n",
    "| `meta.json`                        | JSON storing all related meta-information                           |\n",
    "| `tree_hier_after_merging.html`     | HTML visualization for hierarchical annotation (post-merging)        |\n",
    "\n",
    "### Directories and Their Contents:\n",
    "\n",
    "- **part_renders_after_merging/**:\n",
    "    - Contains rendered images supporting the HTML visualization post-merging.\n",
    "\n",
    "- **point_sample/**: Contains data for point cloud learning (except removed files).\n",
    "    - `sample-points-all-pts-nor-rgba-10000.txt`: Point cloud from whole ShapeNet model with labels transferred.\n",
    "    - `sample-points-all-label-10000.txt`: Labels propagated to `sample-points-all-pts-nor-rgba-10000.txt`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
