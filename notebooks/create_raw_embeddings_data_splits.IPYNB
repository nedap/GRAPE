{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have a fair data proportions experiment, we need to construct the k folds of the Graph dataset beforehand. We will do this here. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### SET PATH TO BE ROOT ###\n",
    "\n",
    "import os\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Move one directory up\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "# Change the working directory\n",
    "os.chdir(parent_dir)\n",
    "# Verify the change\n",
    "print(\"Current working directory:\", os.getcwd())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from easydict import EasyDict\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch.utils.data import Subset\n",
    "from torch_geometric.loader import DataLoader as GeomDataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from models.pytorch_lightning import MAELightningModule, GAELightningModule\n",
    "from datasets.pytorch import GraphDataset\n",
    "from datasets.pytorch_lightning import GNNDataModule, PartNetDataModule, CeasarDataModule, PartNetEmbeddingsDataModule\n",
    "from models.pytorch_models import Point_MAE\n",
    "from utils.config import *\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# args = argparse.Namespace(config='cfgs/build_gnn_ds.yaml', wandb=False)\n",
    "args = argparse.Namespace(config='cfgs/build_gnn_ds_ceasar.yaml', wandb=False)\n",
    "\n",
    "cfg = get_cfg(args=args, logger=None) \n",
    "\n",
    "# Load and freeze the encoder\n",
    "pretrained_frozen_encoder =  MAELightningModule.load_and_freeze_encoder(cfg.group_and_encode_model.pretrained_ckpnt, cfg, args)\n",
    "group_and_encode = GAELightningModule(cfg, args=args, pretrained_encoder=pretrained_frozen_encoder, base_type=False)\n",
    "# load the data module\n",
    "# data_module = PartNetDataModule(cfg=cfg, args=args)\n",
    "data_module = CeasarDataModule(cfg=cfg, args=args)\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=[int(cfg.device.device_id)], \n",
    "    max_epochs=1, # one epoch to extract all latents\n",
    "    logger=None, \n",
    "    # default_root_dir=args.experiment_path,\n",
    "    # limit_val_batches=0,\n",
    ")\n",
    "\n",
    "# fit the model\n",
    "predictions = trainer.predict(model=group_and_encode, datamodule=data_module)\n",
    "\n",
    "# loop through all predictions\n",
    "print(\"creating dataset.\")\n",
    "sample = 0\n",
    "for encoded_batch, labels_batch in tqdm(predictions):\n",
    "    for encoded_pcd, labels in zip(encoded_batch, labels_batch):\n",
    "        file_name = str(sample)\n",
    "        sample += 1\n",
    "        save_dir = Path(f\"/srv/healthcare/datascience/data/GRAPE/caesar_raw_embeddings/{sample}\")\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        # save the data\n",
    "        encoded_pcd = encoded_pcd.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        labels = labels - 1\n",
    "\n",
    "        np.save(save_dir / 'embeddings.npy', encoded_pcd)\n",
    "        np.save(save_dir / 'labels.npy', labels)\n",
    "\n",
    "# print(f'{len([p for p in graph_data_path.iterdir() if p.is_dir()])} graphs created')\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the below code will output something like:\n",
    "\n",
    "2024-06-13 10:34:17,478 - GraphData - INFO - [DATASET] 5159 instances were loaded\n",
    "\n",
    "This can be ignored, because we acces the \"full_dataset\" below, not the splits. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "args = argparse.Namespace(config='cfgs/train_gnn.yaml')\n",
    "\n",
    "cfg = get_cfg(args=args, logger=None)\n",
    "# cfg.dataset.train.return_raw_data = True\n",
    "\n",
    "# init data module\n",
    "data_module = GNNDataModule(cfg=cfg, args=None)\n",
    "full_dataset = data_module.full_dataset\n",
    "\n",
    "# make a 90% train+val and 10% test split\n",
    "train_indices, test_indices = train_test_split(list(range(len(full_dataset))), test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "# make a 5 folds for \n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# create 5 folds, then save each fold as graph_fold_x\n",
    "fold = 0\n",
    "for train_idx, val_idx in kf.split(train_dataset):\n",
    "    # save_dir = Path(f\"/srv/healthcare/datascience/data/GRAPE/table/folds/fold_{fold}\")\n",
    "    save_dir = Path(f\"/srv/healthcare/datascience/data/GRAPE/table_occluded/folds/fold_{fold}\")\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_fold = Subset(train_dataset, train_idx)\n",
    "    val_fold = Subset(train_dataset, val_idx)\n",
    "    \n",
    "    # save training split\n",
    "    for s, idx in enumerate(train_idx):\n",
    "        sample = full_dataset[idx]     \n",
    "        sample_folder = save_dir / 'training' / f\"{s}/\"\n",
    "        sample_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        nodes = sample['nodes']\n",
    "        edges = sample['edges']\n",
    "\n",
    "        # Save the sample\n",
    "        np.save(sample_folder / 'nodes.npy', nodes)\n",
    "        np.save(sample_folder / 'hierarchy_edges.npy', edges)\n",
    "\n",
    "    # save test split\n",
    "    for s, idx in enumerate(val_idx):\n",
    "        sample = full_dataset[idx]     \n",
    "        sample_folder = save_dir / 'validation' / f\"{s}/\"\n",
    "        sample_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        nodes = sample['nodes']\n",
    "        edges = sample['edges']\n",
    "\n",
    "        # Save the sample\n",
    "        np.save(sample_folder / 'nodes.npy', nodes)\n",
    "        np.save(sample_folder / 'hierarchy_edges.npy', edges)\n",
    "\n",
    "    print(f\"Fold {fold} saved\")\n",
    "    fold += 1\n",
    "\n",
    "# Save test data separately\n",
    "# test_save_dir = Path(\"/srv/healthcare/datascience/data/GRAPE/table/test\")\n",
    "test_save_dir = Path(\"/srv/healthcare/datascience/data/GRAPE/table_occluded/test\")\n",
    "\n",
    "test_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for s, idx in enumerate(test_indices):\n",
    "    sample = full_dataset[idx]\n",
    "    sample_folder = test_save_dir / f\"{s}\"\n",
    "    sample_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    nodes = sample['nodes']\n",
    "    edges = sample['edges']\n",
    "\n",
    "    # Save the sample\n",
    "    np.save(sample_folder / 'nodes.npy', nodes)\n",
    "    np.save(sample_folder / 'hierarchy_edges.npy', edges)\n",
    "\n",
    "print(\"Test data saved\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "args = argparse.Namespace(config='cfgs/train_mlp_caesar.yaml')\n",
    "\n",
    "cfg = get_cfg(args=args, logger=None)\n",
    "# cfg.dataset.train.return_raw_data = True\n",
    "\n",
    "# init data module\n",
    "data_module = PartNetEmbeddingsDataModule(cfg=cfg, args=None)\n",
    "full_dataset = data_module.full_dataset\n",
    "\n",
    "# make a 90% train+val and 10% test split\n",
    "train_indices, test_indices = train_test_split(list(range(len(full_dataset))), test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "# make a 5 folds for \n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# create 5 folds, then save each fold as graph_fold_x\n",
    "fold = 0\n",
    "for train_idx, val_idx in kf.split(train_dataset):\n",
    "    # save_dir = Path(f\"/srv/healthcare/datascience/data/GRAPE/table/folds/fold_{fold}\")\n",
    "    save_dir = Path(f\"/srv/healthcare/datascience/data/GRAPE/caesar_raw_embeddings_folds/folds/fold_{fold}\")\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_fold = Subset(train_dataset, train_idx)\n",
    "    val_fold = Subset(train_dataset, val_idx)\n",
    "    \n",
    "    # save training split\n",
    "    for s, idx in enumerate(train_idx):\n",
    "        sample = full_dataset[idx]     \n",
    "        sample_folder = save_dir / 'training' / f\"{s}/\"\n",
    "        sample_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        encoded_pcd, labels = sample\n",
    "\n",
    "        # Save the sample\n",
    "        np.save(sample_folder / 'embeddings.npy', encoded_pcd)\n",
    "        np.save(sample_folder / 'labels.npy', labels)\n",
    "\n",
    "    # save test split\n",
    "    for s, idx in enumerate(val_idx):\n",
    "        sample = full_dataset[idx]     \n",
    "        sample_folder = save_dir / 'validation' / f\"{s}/\"\n",
    "        sample_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        encoded_pcd, labels = sample\n",
    "\n",
    "        # Save the sample\n",
    "        np.save(sample_folder / 'embeddings.npy', encoded_pcd)\n",
    "        np.save(sample_folder / 'labels.npy', labels)\n",
    "\n",
    "    print(f\"Fold {fold} saved\")\n",
    "    fold += 1\n",
    "\n",
    "# Save test data separately\n",
    "# test_save_dir = Path(\"/srv/healthcare/datascience/data/GRAPE/table/test\")\n",
    "test_save_dir = Path(\"/srv/healthcare/datascience/data/GRAPE/caesar_raw_embeddings_folds/test\")\n",
    "\n",
    "test_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for s, idx in enumerate(test_indices):\n",
    "    sample = full_dataset[idx]\n",
    "    sample_folder = test_save_dir / f\"{s}\"\n",
    "    sample_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    encoded_pcd, labels = sample\n",
    "\n",
    "    # Save the sample\n",
    "    np.save(sample_folder / 'embeddings.npy', encoded_pcd)\n",
    "    np.save(sample_folder / 'labels.npy', labels)\n",
    "print(\"Test data saved\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
