{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have a fair data proportions experiment, we need to construct the k folds of the Graph dataset beforehand. We will do this here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/toy-problem\n"
     ]
    }
   ],
   "source": [
    "### SET PATH TO BE ROOT ###\n",
    "\n",
    "import os\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Move one directory up\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "# Change the working directory\n",
    "os.chdir(parent_dir)\n",
    "# Verify the change\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from easydict import EasyDict\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch.utils.data import Subset\n",
    "from torch_geometric.loader import DataLoader as GeomDataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from models.pytorch_lightning import MAELightningModule, GAELightningModule\n",
    "from datasets.pytorch import GraphDataset\n",
    "from datasets.pytorch_lightning import GNNDataModule, PartNetDataModule, CeasarDataModule, PartNetEmbeddingsDataModule\n",
    "from models.pytorch_models import Point_MAE\n",
    "from utils.config import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-11 17:38:54,149 - Point_MAE - INFO - [Point_MAE] \n",
      "2024-08-11 17:38:54,153 - Transformer - INFO - [args] {'mask_ratio': 0.6, 'mask_type': 'rand', 'trans_dim': 384, 'encoder_dims': 384, 'depth': 12, 'drop_path_rate': 0.1, 'num_heads': 6, 'decoder_depth': 4, 'decoder_num_heads': 6}\n",
      "2024-08-11 17:38:54,479 - Point_MAE - INFO - [Point_MAE] divide point cloud into G256 x S32 points ...\n",
      "2024-08-11 17:38:54,974 - Transformer - INFO - [args] {'trans_dim': 384, 'encoder_dims': 384, 'depth': 12, 'drop_path_rate': 0.1, 'num_heads': 6}\n",
      "2024-08-11 17:38:55,200 - GroupAndEncode - INFO - [GroupAndEncode] divide point cloud into G256 x S32 points ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2024-08-11 17:38:55,224 - Ceasar - INFO - [DATASET] 80 instances were loaded\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRETRAINED ENCODER LOADED SUCCESFULLY\n",
      "FINAL MASK RATIO:  0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8a0f5ad284437ca88487288c48dcef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 124.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# args = argparse.Namespace(config='cfgs/build_gnn_ds.yaml', wandb=False)\n",
    "args = argparse.Namespace(config='cfgs/build_gnn_ds_ceasar.yaml', wandb=False)\n",
    "\n",
    "cfg = get_cfg(args=args, logger=None) \n",
    "\n",
    "# Load and freeze the encoder\n",
    "pretrained_frozen_encoder =  MAELightningModule.load_and_freeze_encoder(cfg.group_and_encode_model.pretrained_ckpnt, cfg, args)\n",
    "group_and_encode = GAELightningModule(cfg, args=args, pretrained_encoder=pretrained_frozen_encoder, base_type=False)\n",
    "# load the data module\n",
    "# data_module = PartNetDataModule(cfg=cfg, args=args)\n",
    "data_module = CeasarDataModule(cfg=cfg, args=args)\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=[int(cfg.device.device_id)], \n",
    "    max_epochs=1, # one epoch to extract all latents\n",
    "    logger=None, \n",
    "    # default_root_dir=args.experiment_path,\n",
    "    # limit_val_batches=0,\n",
    ")\n",
    "\n",
    "# fit the model\n",
    "predictions = trainer.predict(model=group_and_encode, datamodule=data_module)\n",
    "\n",
    "# loop through all predictions\n",
    "print(\"creating dataset.\")\n",
    "sample = 0\n",
    "for encoded_batch, labels_batch in tqdm(predictions):\n",
    "    for encoded_pcd, labels in zip(encoded_batch, labels_batch):\n",
    "        file_name = str(sample)\n",
    "        sample += 1\n",
    "        save_dir = Path(f\"/srv/healthcare/datascience/data/GRAPE/caesar_raw_embeddings/{sample}\")\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        # save the data\n",
    "        encoded_pcd = encoded_pcd.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        labels = labels - 1\n",
    "\n",
    "        np.save(save_dir / 'embeddings.npy', encoded_pcd)\n",
    "        np.save(save_dir / 'labels.npy', labels)\n",
    "\n",
    "# print(f'{len([p for p in graph_data_path.iterdir() if p.is_dir()])} graphs created')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the below code will output something like:\n",
    "\n",
    "2024-06-13 10:34:17,478 - GraphData - INFO - [DATASET] 5159 instances were loaded\n",
    "\n",
    "This can be ignored, because we acces the \"full_dataset\" below, not the splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-11 17:32:06,823 - GraphData - INFO - [DATASET] 5159 instances were loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA SPLIT: 3611 train, 1031 val, 517 test samples\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     edges \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medges\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Save the sample\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnodes.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(sample_folder \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhierarchy_edges.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, edges)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# save test split\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/mlp/lib/python3.11/site-packages/numpy/lib/npyio.py:544\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    541\u001b[0m         file \u001b[38;5;241m=\u001b[39m file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    542\u001b[0m     file_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 544\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile_ctx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfid\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43marr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfix_imports\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfix_imports\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(config='cfgs/train_gnn.yaml')\n",
    "\n",
    "cfg = get_cfg(args=args, logger=None)\n",
    "# cfg.dataset.train.return_raw_data = True\n",
    "\n",
    "# init data module\n",
    "data_module = GNNDataModule(cfg=cfg, args=None)\n",
    "full_dataset = data_module.full_dataset\n",
    "\n",
    "# make a 90% train+val and 10% test split\n",
    "train_indices, test_indices = train_test_split(list(range(len(full_dataset))), test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "# make a 5 folds for \n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# create 5 folds, then save each fold as graph_fold_x\n",
    "fold = 0\n",
    "for train_idx, val_idx in kf.split(train_dataset):\n",
    "    # save_dir = Path(f\"/srv/healthcare/datascience/data/GRAPE/table/folds/fold_{fold}\")\n",
    "    save_dir = Path(f\"/srv/healthcare/datascience/data/GRAPE/table_occluded/folds/fold_{fold}\")\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_fold = Subset(train_dataset, train_idx)\n",
    "    val_fold = Subset(train_dataset, val_idx)\n",
    "    \n",
    "    # save training split\n",
    "    for s, idx in enumerate(train_idx):\n",
    "        sample = full_dataset[idx]     \n",
    "        sample_folder = save_dir / 'training' / f\"{s}/\"\n",
    "        sample_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        nodes = sample['nodes']\n",
    "        edges = sample['edges']\n",
    "\n",
    "        # Save the sample\n",
    "        np.save(sample_folder / 'nodes.npy', nodes)\n",
    "        np.save(sample_folder / 'hierarchy_edges.npy', edges)\n",
    "\n",
    "    # save test split\n",
    "    for s, idx in enumerate(val_idx):\n",
    "        sample = full_dataset[idx]     \n",
    "        sample_folder = save_dir / 'validation' / f\"{s}/\"\n",
    "        sample_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        nodes = sample['nodes']\n",
    "        edges = sample['edges']\n",
    "\n",
    "        # Save the sample\n",
    "        np.save(sample_folder / 'nodes.npy', nodes)\n",
    "        np.save(sample_folder / 'hierarchy_edges.npy', edges)\n",
    "\n",
    "    print(f\"Fold {fold} saved\")\n",
    "    fold += 1\n",
    "\n",
    "# Save test data separately\n",
    "# test_save_dir = Path(\"/srv/healthcare/datascience/data/GRAPE/table/test\")\n",
    "test_save_dir = Path(\"/srv/healthcare/datascience/data/GRAPE/table_occluded/test\")\n",
    "\n",
    "test_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for s, idx in enumerate(test_indices):\n",
    "    sample = full_dataset[idx]\n",
    "    sample_folder = test_save_dir / f\"{s}\"\n",
    "    sample_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    nodes = sample['nodes']\n",
    "    edges = sample['edges']\n",
    "\n",
    "    # Save the sample\n",
    "    np.save(sample_folder / 'nodes.npy', nodes)\n",
    "    np.save(sample_folder / 'hierarchy_edges.npy', edges)\n",
    "\n",
    "print(\"Test data saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-11 17:39:03,866 - PartNetEmbeddings - INFO - [DATASET] 80 instances were loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA SPLIT: 56 train, 16 val, 8 test samples\n",
      "Fold 0 saved\n",
      "Fold 1 saved\n",
      "Fold 2 saved\n",
      "Fold 3 saved\n",
      "Fold 4 saved\n",
      "Test data saved\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(config='cfgs/train_mlp_caesar.yaml')\n",
    "\n",
    "cfg = get_cfg(args=args, logger=None)\n",
    "# cfg.dataset.train.return_raw_data = True\n",
    "\n",
    "# init data module\n",
    "data_module = PartNetEmbeddingsDataModule(cfg=cfg, args=None)\n",
    "full_dataset = data_module.full_dataset\n",
    "\n",
    "# make a 90% train+val and 10% test split\n",
    "train_indices, test_indices = train_test_split(list(range(len(full_dataset))), test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "# make a 5 folds for \n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# create 5 folds, then save each fold as graph_fold_x\n",
    "fold = 0\n",
    "for train_idx, val_idx in kf.split(train_dataset):\n",
    "    # save_dir = Path(f\"/srv/healthcare/datascience/data/GRAPE/table/folds/fold_{fold}\")\n",
    "    save_dir = Path(f\"/srv/healthcare/datascience/data/GRAPE/caesar_raw_embeddings_folds/folds/fold_{fold}\")\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_fold = Subset(train_dataset, train_idx)\n",
    "    val_fold = Subset(train_dataset, val_idx)\n",
    "    \n",
    "    # save training split\n",
    "    for s, idx in enumerate(train_idx):\n",
    "        sample = full_dataset[idx]     \n",
    "        sample_folder = save_dir / 'training' / f\"{s}/\"\n",
    "        sample_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        encoded_pcd, labels = sample\n",
    "\n",
    "        # Save the sample\n",
    "        np.save(sample_folder / 'embeddings.npy', encoded_pcd)\n",
    "        np.save(sample_folder / 'labels.npy', labels)\n",
    "\n",
    "    # save test split\n",
    "    for s, idx in enumerate(val_idx):\n",
    "        sample = full_dataset[idx]     \n",
    "        sample_folder = save_dir / 'validation' / f\"{s}/\"\n",
    "        sample_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        encoded_pcd, labels = sample\n",
    "\n",
    "        # Save the sample\n",
    "        np.save(sample_folder / 'embeddings.npy', encoded_pcd)\n",
    "        np.save(sample_folder / 'labels.npy', labels)\n",
    "\n",
    "    print(f\"Fold {fold} saved\")\n",
    "    fold += 1\n",
    "\n",
    "# Save test data separately\n",
    "# test_save_dir = Path(\"/srv/healthcare/datascience/data/GRAPE/table/test\")\n",
    "test_save_dir = Path(\"/srv/healthcare/datascience/data/GRAPE/caesar_raw_embeddings_folds/test\")\n",
    "\n",
    "test_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for s, idx in enumerate(test_indices):\n",
    "    sample = full_dataset[idx]\n",
    "    sample_folder = test_save_dir / f\"{s}\"\n",
    "    sample_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    encoded_pcd, labels = sample\n",
    "\n",
    "    # Save the sample\n",
    "    np.save(sample_folder / 'embeddings.npy', encoded_pcd)\n",
    "    np.save(sample_folder / 'labels.npy', labels)\n",
    "print(\"Test data saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
